{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Instructions on how to use this notebook</h3>\n",
    "Note: this configuration takes as input a config file and outputs features (X) and label (y) ready to be used by a model\n",
    "1. Download the imbalance-learn package:\n",
    "<ul> Open an anaconda Prompt </ul>\n",
    "<ul> Run <code>pip install -U imbalanced-learn</code> </ul>\n",
    "\n",
    "1. Configure your paramters using the transform_config.ini config file\n",
    "<ul> sql_query = the sql query to fetch the paramters of interest (features + labels) from the database </ul>\n",
    "<ul>columns = list of column names that were used in the sql_query SELECT clause, will need it to load the data of the sql_query into a pandas dataframe </ul>\n",
    "<ul> categorical_col_list = the list of column names from columns that are categorical (these will need to be encoded) </ul>\n",
    "<ul> numeric_col_list = the list of column names from columns that are numerical (these will need to be normalized) </ul>\n",
    "<ul> label = the label you want your model to learn to predict </ul>\n",
    "<ul> labels_to_drop = list of column names that were fetched by the sql query but that you don't want to use as features (will be dropped during extraction) - optional - </ul>\n",
    "<ul> top_k = top k features to use for the model </ul>\n",
    "2. Run the notebook and call preprocess_and_transform_data which returns X and y of your data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import configparser\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from collections import Counter \n",
    "from sklearn import preprocessing\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.preprocessing import OrdinalEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config(filename='database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    "\n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish the connection and create a curose to the database \n",
    "def connect(cfg):\n",
    "    try:\n",
    "        print(\"Attempting to connect to the database\")\n",
    "        conn = psycopg2.connect(**cfg)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected!\")\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    return conn, cursor \n",
    "\n",
    "def close_connection(connection, cursor):\n",
    "    print(\"Closing connection\")\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"Connection closed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(cursor, query):\n",
    "    \"\"\"\n",
    "    Fetch the data from the db \n",
    "    \"\"\"\n",
    "    print(\"Fetching query...\")\n",
    "    # Get the features and labels\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        #Get the complete result set: list of tuples where each tuple is a row from the result set \n",
    "        result_list = cursor.fetchall()\n",
    "        print(\"Fetched!\")\n",
    "        return result_list\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(result_list, cols):\n",
    "    \"\"\"\n",
    "    Load data in a dataframe\n",
    "    \"\"\"\n",
    "    print(\"Loading data in dataframe\")\n",
    "    result_df = pd.DataFrame(result_list, columns=cols)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separate labels and features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_and_features(df, label_name, labels_to_drop):\n",
    "    \"\"\"\n",
    "    Return the label df and delete all labels from the original df\n",
    "    Returns the label df and a copy of the original df without labels\n",
    "    \"\"\"\n",
    "    print(\"Extracting labels and features...\")\n",
    "    y = df[label_name]\n",
    "    # Normalize y \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y) \n",
    "    label_list = list(le.classes_)\n",
    "    # Drop all the labels from \n",
    "    features_df = df.drop(columns=[label_name], inplace=False)\n",
    "    for label in labels_to_drop:\n",
    "        features_df = features_df.drop(columns=[label], inplace=False)\n",
    "    print(\"Labels and features extracted!\")\n",
    "    return features_df, y, label_list\n",
    "\n",
    "def encode(df,cat_cols_list):\n",
    "    \"\"\"\n",
    "    Uses ordinal encoder to encode the cat_cols_list\n",
    "    \"\"\"\n",
    "    print(\"Encoding categorical features...\")\n",
    "    enc = OrdinalEncoder()\n",
    "    df_enc = enc.fit_transform(df)\n",
    "    df_encoded = pd.DataFrame(df_enc, columns=df.columns)\n",
    "    print(enc.categories_)\n",
    "    return df_encoded \n",
    "\n",
    "def normalize(df,numeric_col_list):\n",
    "    \"\"\"\n",
    "    Normalizes the numeric columns\n",
    "    \"\"\"\n",
    "    print(\"Normalizing numerical features...\")\n",
    "    # Create a min-max processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # Create an object to transform the data to fit minmax processor \n",
    "    df_scaled = min_max_scaler.fit_transform(df)\n",
    "    # Run normalized on the dataframe\n",
    "    df_normalized = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "    return df_normalized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, label_name, labels_to_drop, categorical_col_list, numeric_col_list):\n",
    "    \"\"\"\n",
    "    Given the dataframe, label, labels to drop, list of categorical and numerical columns will\n",
    "    (1) Extract the label of interest\n",
    "    (2) Encode categorical columns\n",
    "    (3) Normalize numerical columns\n",
    "    \"\"\"\n",
    "    print(\"Started preprocessing...\")\n",
    "    # Convert numeric cols to floats \n",
    "    for col in numeric_col_list:\n",
    "        \n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    features_df, y, label_list = get_label_and_features(df, label_name, labels_to_drop)\n",
    "    features_df = encode(features_df, categorical_col_list)\n",
    "    # Get the encoding mapping to be able to interpret model let \n",
    "    # Get the non-encoded columns\n",
    "    # Get the encoded columns \n",
    "    # Create \n",
    "    num_normalized = normalize(features_df, numeric_col_list)\n",
    "    print(\"Preprocessing done!\")\n",
    "    return num_normalized.values, y, num_normalized.columns, label_list, num_normalized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Perform feature selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, y_train, X_test, top_k, feature_col):\n",
    "    \"\"\"\n",
    "    Perform feature selection using the mutual information algorithm \n",
    "    \"\"\"\n",
    "    print(\"Started feature selection...\")\n",
    "    print(f\"Selecting top {top_k} features\")\n",
    "    select_k_best_classif = SelectKBest(chi2, k=top_k)\n",
    "    select_k_best_classif.fit(X_train, y_train)\n",
    "    X_train_new = select_k_best_classif.transform(X_train)\n",
    "    X_test_new = select_k_best_classif.transform(X_test)\n",
    "    # Print the selected features in order\n",
    "    mask = select_k_best_classif.get_support() # list of booleans \n",
    "    new_features = [] # list of k best features\n",
    "    for bool, feature in zip(mask, feature_col):\n",
    "        if bool:\n",
    "            new_features.append(feature)\n",
    "    print(\"Feature selection done!\")\n",
    "    return X_train_new, X_test_new, new_features\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def tree_feature_select(X_train, y_train, X_test, top_k, feature_col):\n",
    "    print(\"Started tree feature selection...\")\n",
    "    clf = ExtraTreesClassifier(n_estimators=50)\n",
    "    clf = clf.fit(X, y)\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    X_train_new = model.transform(X_train)\n",
    "    X_test_new = model.transform(X_test)\n",
    "    print(\"Feature selection done!\")\n",
    "    return X_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notebook entry point__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to the preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database and load data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file):\n",
    "    # Read the config file \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    # Clean up sql_query \n",
    "    sql_query = config['DEFAULT']['sql_query'].replace('\\n', ' ').strip()\n",
    "    columns = config['DEFAULT']['columns'].replace('\\n', ' ').replace('\\n', ' ').split(', ')\n",
    "    categorical_col_list = config['DEFAULT']['categorical_col_list'].replace('\\n', ' ').split(', ')\n",
    "    numeric_col_list = config['DEFAULT']['numeric_col_list'].replace('\\n', ' ').split(', ')\n",
    "    label = config['DEFAULT']['label']\n",
    "    labels_to_drop = config['DEFAULT']['labels_to_drop']\n",
    "    if labels_to_drop:\n",
    "        labels_to_drop = config['DEFAULT']['labels_to_drop'].split(', ')\n",
    "    else:\n",
    "        labels_to_drop = []\n",
    "    top_k = int(config['DEFAULT']['top_k'])\n",
    "    return sql_query, columns, categorical_col_list, numeric_col_list, label, labels_to_drop, top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    # Get the configuration file as a python dict\n",
    "    cfg = config()\n",
    "    # Connect to the database\n",
    "    conn, cursor = connect(cfg)\n",
    "    # Fetch the data\n",
    "    result_list = fetch(cursor, sql_query)\n",
    "    # Load results in datafram\n",
    "    result_df = load(result_list, columns)\n",
    "    print(\"Data loaded into dataframe!\")\n",
    "    print(f\"Class imbalance check for label: {label}\")\n",
    "    print(Counter(result_df[label]))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(result_df, top_k):\n",
    "    print(\"Started data transformation...\")\n",
    "\n",
    "    # extract labels and features and preprocess features: normalize and encode\n",
    "    X, y, feature_col, label_list, normalized_df = preprocess(result_df, label, labels_to_drop,categorical_col_list, numeric_col_list)\n",
    "\n",
    "    # Perform feature selection \n",
    "#     X = select_features(X, y, top_k, feature_col)\n",
    "#     X = tree_feature_select(X, y, top_k)\n",
    "    print(\"Data transformation done! Data is ready for training.\")\n",
    "    return X, y, feature_col, label_list, normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query, columns, categorical_col_list, numeric_col_list, label, labels_to_drop, top_k = load_config('transform_config.ini')\n",
    "\n",
    "def preprocess_and_transform_data():\n",
    "    result_df = load_data()\n",
    "    X, y, feature_col, label_list, normalized_df = data_transform(result_df, top_k)\n",
    "    return X, y, feature_col, label_list, normalized_df, result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
